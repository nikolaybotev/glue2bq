#!/usr/bin/env python3
"""
Create a proper Iceberg table with metadata files for BigQuery Omni compatibility.
This script creates the actual Iceberg metadata structure that BigQuery Omni expects.
"""

import boto3
import json
import sys
import os
import uuid
from datetime import datetime, timedelta
import random
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path

def get_glue_client(aws_region):
    """Create AWS Glue client"""
    return boto3.client('glue', region_name=aws_region)

def get_s3_client(aws_region):
    """Create S3 client"""
    return boto3.client('s3', region_name=aws_region)

def create_sample_data(num_records=1000):
    """Create sample data with date partitioning"""
    data = []
    base_date = datetime(2024, 1, 1)
    
    for i in range(num_records):
        # Generate random date within the last 30 days
        random_days = random.randint(0, 30)
        record_date = base_date + timedelta(days=random_days)
        
        data.append({
            'id': i + 1,
            'name': f'Record_{i + 1}',
            'value': round(random.uniform(10.0, 100.0), 2),
            'category': random.choice(['A', 'B', 'C', 'D']),
            'date': record_date.strftime('%Y-%m-%d')
        })
    
    return pd.DataFrame(data)

def create_parquet_files_with_partitioning(df, table_name, bucket_name, aws_region):
    """Create Parquet files with date partitioning structure"""
    
    # Group data by date
    grouped = df.groupby('date')
    
    print("Creating Parquet files with date partitioning...")
    
    local_path = f"/tmp/{table_name}"
    os.makedirs(local_path, exist_ok=True)
    
    manifest_files = []
    
    for date, group_df in grouped:
        # Convert to PyArrow table
        table = pa.Table.from_pandas(group_df)
        
        # Create partition directory structure
        partition_path = f"{local_path}/date={date}"
        os.makedirs(partition_path, exist_ok=True)
        
        # Write Parquet file
        parquet_file = f"{partition_path}/data.parquet"
        pq.write_table(table, parquet_file)
        
        # Track for manifest
        manifest_files.append({
            'path': f"s3://{bucket_name}/iceberg/{table_name}/date={date}/data.parquet",
            'length': os.path.getsize(parquet_file),
            'partition': f"date={date}"
        })
        
        print(f"Created: {parquet_file} with {len(group_df)} records")
    
    print(f"Parquet files created in {local_path}/")
    
    # Upload to S3
    print("Uploading to S3...")
    os.system(f"aws s3 sync {local_path}/ s3://{bucket_name}/iceberg/{table_name}/ --region {aws_region}")
    
    print(f"‚úÖ Data uploaded to s3://{bucket_name}/iceberg/{table_name}/")
    
    return local_path, manifest_files

def create_iceberg_metadata(bucket_name, table_name, manifest_files):
    """Create proper Iceberg metadata.json file"""
    
    table_uuid = str(uuid.uuid4())
    current_time_ms = int(datetime.now().timestamp() * 1000)
    
    # Create manifest list entry
    manifest_list_entry = {
        "manifest_path": f"s3://{bucket_name}/iceberg/{table_name}/metadata/manifest-list-{current_time_ms}.avro",
        "manifest_length": 1024,  # Placeholder
        "partition_spec_id": 0,
        "added_snapshot_id": 1,
        "added_data_files_count": len(manifest_files),
        "added_rows_count": sum(f.get('length', 0) for f in manifest_files),
        "existing_data_files_count": 0,
        "existing_rows_count": 0,
        "deleted_data_files_count": 0,
        "deleted_rows_count": 0
    }
    
    # Create snapshot
    snapshot = {
        "snapshot-id": 1,
        "timestamp-ms": current_time_ms,
        "summary": {
            "operation": "append",
            "added-data-files": str(len(manifest_files)),
            "added-records": str(len(manifest_files) * 30),  # Approximate
            "total-records": str(len(manifest_files) * 30)
        },
        "manifest-list": f"s3://{bucket_name}/iceberg/{table_name}/metadata/manifest-list-{current_time_ms}.avro",
        "schema-id": 0
    }
    
    # Create the main metadata.json
    metadata = {
        "format-version": 2,
        "table-uuid": table_uuid,
        "location": f"s3://{bucket_name}/iceberg/{table_name}/",
        "last-updated-ms": current_time_ms,
        "last-column-id": 5,
        "last-sequence-number": 0,
        "schema": {
            "type": "struct",
            "schema-id": 0,
            "fields": [
                {
                    "id": 1,
                    "name": "id",
                    "required": True,
                    "type": "long"
                },
                {
                    "id": 2,
                    "name": "name",
                    "required": True,
                    "type": "string"
                },
                {
                    "id": 3,
                    "name": "value",
                    "required": True,
                    "type": "double"
                },
                {
                    "id": 4,
                    "name": "category",
                    "required": True,
                    "type": "string"
                },
                {
                    "id": 5,
                    "name": "date",
                    "required": True,
                    "type": "date"
                }
            ]
        },
        "current-schema-id": 0,
        "schemas": [
            {
                "type": "struct",
                "schema-id": 0,
                "fields": [
                    {
                        "id": 1,
                        "name": "id",
                        "required": True,
                        "type": "long"
                    },
                    {
                        "id": 2,
                        "name": "name",
                        "required": True,
                        "type": "string"
                    },
                    {
                        "id": 3,
                        "name": "value",
                        "required": True,
                        "type": "double"
                    },
                    {
                        "id": 4,
                        "name": "category",
                        "required": True,
                        "type": "string"
                    },
                    {
                        "id": 5,
                        "name": "date",
                        "required": True,
                        "type": "date"
                    }
                ]
            }
        ],
        "partition-spec": [
            {
                "name": "date",
                "transform": "identity",
                "source-id": 5,
                "field-id": 1000
            }
        ],
        "default-spec-id": 0,
        "partition-specs": [
            {
                "spec-id": 0,
                "fields": [
                    {
                        "name": "date",
                        "transform": "identity",
                        "source-id": 5,
                        "field-id": 1000
                    }
                ]
            }
        ],
        "last-partition-id": 1000,
        "default-sort-order-id": 0,
        "sort-orders": [
            {
                "order-id": 0,
                "fields": []
            }
        ],
        "properties": {
            "owner": "glue2bq",
            "write.format.default": "parquet",
            "write.parquet.compression-codec": "snappy",
            "write.metadata.metrics.default": "truncate(16)",
            "write.metadata.metrics.column.id": "truncate(16)",
            "write.metadata.metrics.column.name": "truncate(16)",
            "write.metadata.metrics.column.value": "truncate(16)",
            "write.metadata.metrics.column.category": "truncate(16)",
            "write.metadata.metrics.column.date": "truncate(16)"
        },
        "current-snapshot-id": 1,
        "refs": {
            "main": {
                "snapshot-id": 1,
                "type": "branch"
            }
        },
        "snapshots": [snapshot],
        "snapshot-log": [
            {
                "timestamp-ms": current_time_ms,
                "snapshot-id": 1
            }
        ],
        "metadata-log": []
    }
    
    return metadata

def upload_metadata(s3_client, bucket_name, table_name, metadata):
    """Upload Iceberg metadata files to S3"""
    
    metadata_path = f"iceberg/{table_name}/metadata/"
    
    # Upload metadata.json
    metadata_key = f"{metadata_path}v1.metadata.json"
    s3_client.put_object(
        Bucket=bucket_name,
        Key=metadata_key,
        Body=json.dumps(metadata, indent=2),
        ContentType='application/json'
    )
    
    print(f"‚úÖ Uploaded metadata: s3://{bucket_name}/{metadata_key}")
    
    return f"s3://{bucket_name}/{metadata_key}"

def create_proper_iceberg_table(glue_client, database_name, table_name, bucket_name, metadata_location):
    """Create proper Iceberg table in AWS Glue"""
    
    table_input = {
        'Name': table_name,
        'Description': 'Proper Iceberg table with metadata for BigQuery Omni',
        'TableType': 'EXTERNAL_TABLE',
        'Parameters': {
            'table_type': 'ICEBERG',
            'metadata_location': metadata_location,
            'write.data.path': f's3://{bucket_name}/iceberg/{table_name}/data/',
            'write.metadata.path': f's3://{bucket_name}/iceberg/{table_name}/metadata/',
            'write.format.default': 'parquet',
            'write.parquet.compression-codec': 'snappy',
            'iceberg.table.version': '2'
        },
        'StorageDescriptor': {
            'Columns': [
                {'Name': 'id', 'Type': 'bigint'},
                {'Name': 'name', 'Type': 'string'},
                {'Name': 'value', 'Type': 'double'},
                {'Name': 'category', 'Type': 'string'},
                {'Name': 'date', 'Type': 'date'}
            ],
            'Location': f's3://{bucket_name}/iceberg/{table_name}/',
            'InputFormat': 'org.apache.iceberg.mr.hive.IcebergInputFormat',
            'OutputFormat': 'org.apache.iceberg.mr.hive.IcebergOutputFormat',
            'SerdeInfo': {
                'SerializationLibrary': 'org.apache.iceberg.mr.hive.HiveIcebergSerDe'
            }
        },
        'PartitionKeys': [
            {'Name': 'date', 'Type': 'date'}
        ]
    }
    
    try:
        # Check if table already exists
        try:
            glue_client.get_table(DatabaseName=database_name, Name=table_name)
            print(f"Table {table_name} already exists. Updating...")
            glue_client.update_table(DatabaseName=database_name, TableInput=table_input)
        except glue_client.exceptions.EntityNotFoundException:
            print(f"Creating new table {table_name}...")
            glue_client.create_table(DatabaseName=database_name, TableInput=table_input)
        
        print(f"‚úÖ Successfully created/updated Iceberg table: {database_name}.{table_name}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error creating Glue table: {str(e)}")
        return False

def main():
    if len(sys.argv) != 4:
        print("Usage: python create_proper_iceberg.py <bucket_name> <aws_region> <table_name>")
        print("Example: python create_proper_iceberg.py glue2bq-dev-data-bucket us-east-1 sample_iceberg_table")
        sys.exit(1)
    
    bucket_name = sys.argv[1]
    aws_region = sys.argv[2]
    table_name = sys.argv[3]
    database_name = "glue2bq-dev_database"
    
    print(f"Creating proper Iceberg table '{table_name}' in database '{database_name}'")
    print(f"Using bucket: {bucket_name} in region: {aws_region}")
    
    # Create clients
    glue_client = get_glue_client(aws_region)
    s3_client = get_s3_client(aws_region)
    
    try:
        # Generate sample data
        print("Generating sample data...")
        df = create_sample_data(num_records=1000)
        print(f"Generated {len(df)} records")
        
        # Create Parquet files with partitioning
        print("Creating Parquet files...")
        local_path, manifest_files = create_parquet_files_with_partitioning(df, table_name, bucket_name, aws_region)
        
        # Create Iceberg metadata
        print("Creating Iceberg metadata...")
        metadata = create_iceberg_metadata(bucket_name, table_name, manifest_files)
        
        # Upload metadata
        print("Uploading metadata...")
        metadata_location = upload_metadata(s3_client, bucket_name, table_name, metadata)
        
        # Create proper Glue table
        print("Creating proper Glue Iceberg table...")
        success = create_proper_iceberg_table(glue_client, database_name, table_name, bucket_name, metadata_location)
        
        if success:
            print(f"\nüéâ Successfully created proper Iceberg table!")
            print(f"Table: {database_name}.{table_name}")
            print(f"Location: s3://{bucket_name}/iceberg/{table_name}/")
            print(f"Metadata: {metadata_location}")
            print(f"Partitioned by: date")
            print(f"Format: Parquet")
            
            print(f"\n‚úÖ BigQuery Omni should now work!")
            print(f"Try this query:")
            print(f'SELECT * FROM "AwsDataCatalog"."{database_name}"."{table_name}" LIMIT 10;')
            
        else:
            print("‚ùå Failed to create proper Iceberg table")
            sys.exit(1)
            
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        sys.exit(1)
    
    finally:
        # Clean up local files
        if 'local_path' in locals():
            os.system(f"rm -rf {local_path}")

if __name__ == "__main__":
    main()
